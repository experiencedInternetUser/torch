{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002d30b8",
   "metadata": {},
   "source": [
    "## Задание 2: Работа с датасетами (30 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ed89e",
   "metadata": {},
   "source": [
    "### 2.1 Кастомный Dataset класс (15 баллов)\n",
    "```python\n",
    "# Создайте кастомный класс датасета для работы с CSV файлами:\n",
    "# - Загрузка данных из файла\n",
    "# - Предобработка (нормализация, кодирование категорий)\n",
    "# - Поддержка различных форматов данных (категориальные, числовые, бинарные и т.д.)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af6041f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9eba9990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_file, numeric_cols, categorical_cols, binary_cols, target_col):\n",
    "        self.df = pd.read_csv(path_file)\n",
    "        self.path_file = path_file\n",
    "        \n",
    "        # Заполнение пропусков — исправлено без inplace\n",
    "        for col in numeric_cols:\n",
    "            self.df[col] = self.df[col].fillna(self.df[col].median())\n",
    "        for col in categorical_cols:\n",
    "            self.df[col] = self.df[col].fillna(self.df[col].mode()[0])\n",
    "        for col in binary_cols:\n",
    "            self.df[col] = self.df[col].fillna(0)\n",
    "\n",
    "        # Обработка числовых признаков\n",
    "        self.scaler = StandardScaler()\n",
    "        numeric_data = self.scaler.fit_transform(self.df[numeric_cols])\n",
    "\n",
    "        # Обработка категориальных признаков\n",
    "        try:\n",
    "            self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        except TypeError:\n",
    "            self.encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "        cat_data = self.encoder.fit_transform(self.df[categorical_cols])\n",
    "        cat_data = np.array(cat_data)\n",
    "\n",
    "        # Обработка бинарных признаков\n",
    "        if binary_cols:\n",
    "            binary_data = self.df[binary_cols].astype(int).values\n",
    "        else:\n",
    "            binary_data = np.empty((len(self.df), 0))\n",
    "\n",
    "        # Обработка целевой переменной\n",
    "        y_raw = self.df[target_col]\n",
    "        if y_raw.dtype == 'object':\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            y = self.label_encoder.fit_transform(y_raw)\n",
    "        else:\n",
    "            y = y_raw.values\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # Объединение всех признаков\n",
    "        X = np.concatenate([numeric_data, cat_data, binary_data], axis=1)\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00584a07",
   "metadata": {},
   "source": [
    "### 2.2 Эксперименты с различными датасетами (15 баллов)\n",
    "```python\n",
    "# Найдите csv датасеты для регрессии и бинарной классификации и, применяя наработки из предыдущей части задания, обучите линейную и логистическую регрессию\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a06e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from linRegAndLogReg import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eac848d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_dataset = CustomDataset(\n",
    "        path_file='train.csv',\n",
    "        numeric_cols=['Age', 'Fare'],\n",
    "        categorical_cols=['Pclass', 'Sex', 'Embarked'],\n",
    "        binary_cols=[],\n",
    "        target_col='Survived'\n",
    ")\n",
    "\n",
    "spotify_dataset = CustomDataset(\n",
    "    path_file='spotify.csv',\n",
    "    numeric_cols=['duration_ms', 'danceability', 'energy', 'loudness', 'speechiness',\n",
    "                    'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo'],\n",
    "    categorical_cols=['mode', 'key', 'time_signature', 'track_genre'],\n",
    "    binary_cols=['explicit'],\n",
    "    target_col='popularity'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60ef7e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression on train.csv\n",
      "Epoch 10/50, Loss: 0.4558\n",
      "Epoch 20/50, Loss: 0.4510\n",
      "Epoch 30/50, Loss: 0.4498\n",
      "Epoch 40/50, Loss: 0.4484\n",
      "Epoch 50/50, Loss: 0.4494\n",
      "Training LinearRegression on spotify.csv\n",
      "Epoch 10/50, Loss: 379.0816\n",
      "Epoch 20/50, Loss: 378.7777\n",
      "Epoch 30/50, Loss: 379.1114\n",
      "Epoch 40/50, Loss: 379.3979\n",
      "Epoch 50/50, Loss: 378.9909\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "def fit_model(dataset, batch_size, model, criterion, optimizer):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(f\"Training {str(model).split('(')[0]} on {dataset.path_file}\")\n",
    "    train_model(model, dataloader, criterion, optimizer)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = LogisticRegression(in_features = titanic_dataset.X.shape[1])\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    fit_model(titanic_dataset, 32, model, criterion, optimizer)\n",
    "    \n",
    "    model = LinearRegression(in_features = spotify_dataset.X.shape[1])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    fit_model(spotify_dataset, 32, model, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
